{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a630f56",
   "metadata": {},
   "source": [
    "# CTSE Lecture Notes Chatbot using Gemini API\n",
    "# SE4010 - Current Trends in Software Engineering Assignment"
   ]
  },
  {

   "cell_type": "code",
   "execution_count": null,
   "id": "651aa459",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-10 16:26:11,845 - ctse_chatbot - INFO - Environment variables loaded successfully\n",
      "2025-05-10 16:26:11,848 - ctse_chatbot - INFO - Loading existing vector store\n",
      "C:\\Users\\Pasan\\AppData\\Local\\Temp\\ipykernel_3872\\307431581.py:266: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n",
      "2025-05-10 16:26:32,779 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "2025-05-10 16:26:38,462 - ctse_chatbot - WARNING - Failed to load vector store: The de-serialization relies loading a pickle file. Pickle files can be modified to deliver a malicious payload that results in execution of arbitrary code on your machine.You will need to set `allow_dangerous_deserialization` to `True` to enable deserialization. If you do this, make sure that you trust the source of the data. For example, if you are loading a file that you created, and know that no one else has modified the file, then this is safe to do. Do not set this to `True` if you are loading a file from an untrusted source (e.g., some random site on the internet.).. Will create new one.\n",
      "2025-05-10 16:26:38,463 - ctse_chatbot - ERROR - Error with vector store: No text chunks provided and no existing vector store found\n",
      "2025-05-10 16:26:38,464 - ctse_chatbot - WARNING - Error loading vector store: No text chunks provided and no existing vector store found. Creating new one.\n",
      "2025-05-10 16:26:38,466 - ctse_chatbot - INFO - Processing PPTX: AWS User Groups Colombo - Introduction to AWS Cloud Platform.pptx\n",
      "2025-05-10 16:26:38,524 - ctse_chatbot - INFO - Processing PPTX: Cloud Computing 101.pptx\n",
      "2025-05-10 16:26:38,549 - ctse_chatbot - INFO - Processing PPTX: Cloud Design Patterns - 1.pptx\n",
      "2025-05-10 16:26:38,580 - ctse_chatbot - INFO - Processing PPTX: Cloud Design Patterns - 2.pptx\n",
      "2025-05-10 16:26:38,617 - ctse_chatbot - INFO - Processing PPTX: Containers 101.pptx\n",
      "2025-05-10 16:26:38,690 - ctse_chatbot - INFO - Processing PPTX: Intro to DevOps and Beyond.pptx\n",
      "2025-05-10 16:26:38,752 - ctse_chatbot - INFO - Processing PPTX: Introduction to Microservices.pptx\n",
      "2025-05-10 16:26:38,777 - ctse_chatbot - INFO - Processing PPTX: Key Essentials for Building Application in Cloud.pptx\n",
      "2025-05-10 16:26:38,835 - ctse_chatbot - INFO - Processing PPTX: Microservice Design Patterns.pptx\n",
      "2025-05-10 16:26:38,868 - ctse_chatbot - INFO - Processing PDF: CAP Theorem.pdf\n",
      "2025-05-10 16:26:39,012 - ctse_chatbot - INFO - Processing PDF: Lecture 2 - Part 1.pdf\n",
      "2025-05-10 16:26:39,170 - ctse_chatbot - INFO - Processing PDF: Lecture 2 - Part 2.pdf\n",
      "2025-05-10 16:26:39,207 - ctse_chatbot - INFO - Processed 12 out of 12 document files\n",
      "2025-05-10 16:26:39,208 - ctse_chatbot - INFO - Loaded 12 document files\n",
      "2025-05-10 16:26:39,209 - ctse_chatbot - INFO - Created 115 text chunks from 12 documents\n",
      "2025-05-10 16:26:39,210 - ctse_chatbot - INFO - Created 115 text chunks\n",
      "2025-05-10 16:26:39,211 - ctse_chatbot - INFO - Initializing embeddings model: all-MiniLM-L6-v2\n",
      "2025-05-10 16:26:39,213 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "2025-05-10 16:26:42,949 - ctse_chatbot - INFO - Creating vector store with 115 chunks\n",
      "2025-05-10 16:26:46,963 - faiss.loader - INFO - Loading faiss with AVX512 support.\n",
      "2025-05-10 16:26:46,964 - faiss.loader - INFO - Could not load library with AVX512 support due to:\n",
      "ModuleNotFoundError(\"No module named 'faiss.swigfaiss_avx512'\")\n",
      "2025-05-10 16:26:46,964 - faiss.loader - INFO - Loading faiss with AVX2 support.\n",
      "2025-05-10 16:26:47,315 - faiss.loader - INFO - Successfully loaded faiss with AVX2 support.\n",
      "2025-05-10 16:26:47,324 - faiss - INFO - Failed to load GPU Faiss: name 'GpuIndexIVFFlat' is not defined. Will not load constructor refs for GPU indexes. This is only an error if you're trying to use GPU Faiss.\n",
      "2025-05-10 16:26:47,331 - ctse_chatbot - INFO - Vector store saved to ./vector_store\\faiss_index\n",
      "C:\\Users\\Pasan\\AppData\\Local\\Temp\\ipykernel_3872\\307431581.py:371: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(\n",
      "2025-05-10 16:26:47,332 - ctse_chatbot - INFO - Setting up Gemini LLM with model: gemini-2.0-flash\n",
      "2025-05-10 16:26:47,408 - ctse_chatbot - INFO - Conversational retrieval chain created successfully\n",
      "2025-05-10 16:26:47,409 - ctse_chatbot - INFO - Chatbot initialized successfully\n",
      "2025-05-10 16:26:47,409 - ctse_chatbot - INFO - Chatbot initialization complete\n",
      "C:\\Users\\Pasan\\AppData\\Local\\Temp\\ipykernel_3872\\307431581.py:490: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(height=500)\n",
      "2025-05-10 16:26:48,384 - ctse_chatbot - INFO - Starting Gradio interface\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-10 16:26:48,717 - httpx - INFO - HTTP Request: GET http://127.0.0.1:7860/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "2025-05-10 16:26:48,868 - httpx - INFO - HTTP Request: HEAD http://127.0.0.1:7860/ \"HTTP/1.1 200 OK\"\n",
      "2025-05-10 16:26:50,031 - httpx - INFO - HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "2025-05-10 16:26:50,292 - httpx - INFO - HTTP Request: GET https://api.gradio.app/v3/tunnel-request \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Could not create share link. Please check your internet connection or our status page: https://status.gradio.app.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-10 16:27:19,376 - ctse_chatbot - INFO - Processing question: explain cap theorem\n",
      "C:\\Users\\Pasan\\AppData\\Local\\Temp\\ipykernel_3872\\307431581.py:400: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = qa_chain({\"question\": message})\n",
      "C:\\Users\\Pasan\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_google_genai\\chat_models.py:357: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n",
      "2025-05-10 16:27:22,809 - ctse_chatbot - INFO - Successfully generated response\n",
      "2025-05-10 16:27:36,218 - ctse_chatbot - INFO - Processing question: explain microservices\n",
      "C:\\Users\\Pasan\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_google_genai\\chat_models.py:357: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n",
      "C:\\Users\\Pasan\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_google_genai\\chat_models.py:357: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n",
      "2025-05-10 16:27:39,080 - ctse_chatbot - INFO - Successfully generated response\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr # user interface\n",
    "import os\n",
    "import glob\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "from pptx import Presentation\n",
    "import PyPDF2  # Add PDF library\n",
    "import re      # For text file processing\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"chatbot.log\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(\"ctse_chatbot\")\n",
    "\n",
    "# Load environment variables from .env file\n",
    "try:\n",
    "    load_dotenv()\n",
    "    GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "    if not GOOGLE_API_KEY:\n",
    "        raise ValueError(\"GOOGLE_API_KEY not found in environment variables\")\n",
    "    \n",
    "    # Configure Gemini API\n",
    "    genai.configure(api_key=GOOGLE_API_KEY)\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
    "    \n",
    "    # Get other environment variables with defaults\n",
    "    VECTOR_STORE_DIR = os.getenv(\"VECTOR_STORE_DIR\", \"./vector_store\")\n",
    "    LECTURES_DIR = os.getenv(\"LECTURES_DIR\", \"./lectures\")\n",
    "    MODEL_NAME = os.getenv(\"MODEL_NAME\", \"gemini-2.0-flash\")\n",
    "    TEMPERATURE = float(os.getenv(\"TEMPERATURE\", \"0.7\"))\n",
    "    TOP_P = float(os.getenv(\"TOP_P\", \"0.85\"))\n",
    "    MAX_TOKENS = int(os.getenv(\"MAX_TOKENS\", \"1024\"))\n",
    "    EMBEDDING_MODEL = os.getenv(\"EMBEDDING_MODEL\", \"all-MiniLM-L6-v2\")\n",
    "    \n",
    "    logger.info(\"Environment variables loaded successfully\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error loading environment variables: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "# Function to extract text from PowerPoint files\n",
    "def extract_text_from_pptx(pptx_path):\n",
    "    \"\"\"Extract text from a PowerPoint file with error handling.\"\"\"\n",
    "    try:\n",
    "        prs = Presentation(pptx_path)\n",
    "        text_content = []\n",
    "        \n",
    "        # Extract slide number for context\n",
    "        slide_number = 1\n",
    "        \n",
    "        for slide in prs.slides:\n",
    "            try:\n",
    "                slide_text = f\"Slide {slide_number}: \"\n",
    "                for shape in slide.shapes:\n",
    "                    if hasattr(shape, \"text\"):\n",
    "                        slide_text += shape.text + \" \"\n",
    "                text_content.append(slide_text.strip())\n",
    "                slide_number += 1\n",
    "            except Exception as slide_error:\n",
    "                logger.warning(f\"Error processing slide {slide_number} in {pptx_path}: {str(slide_error)}\")\n",
    "                text_content.append(f\"Slide {slide_number}: [Error extracting content]\")\n",
    "                slide_number += 1\n",
    "        \n",
    "        return \"\\n\\n\".join(text_content)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to extract text from {pptx_path}: {str(e)}\")\n",
    "        return f\"Error processing file {os.path.basename(pptx_path)}: {str(e)}\"\n",
    "\n",
    "# New function to extract text from PDF files\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extract text from a PDF file with error handling.\"\"\"\n",
    "    try:\n",
    "        text_content = []\n",
    "        \n",
    "        with open(pdf_path, 'rb') as file:\n",
    "            pdf_reader = PyPDF2.PdfReader(file)\n",
    "            page_count = len(pdf_reader.pages)\n",
    "            \n",
    "            for page_num in range(page_count):\n",
    "                try:\n",
    "                    page = pdf_reader.pages[page_num]\n",
    "                    page_text = f\"Page {page_num + 1}: {page.extract_text()}\"\n",
    "                    text_content.append(page_text.strip())\n",
    "                except Exception as page_error:\n",
    "                    logger.warning(f\"Error processing page {page_num + 1} in {pdf_path}: {str(page_error)}\")\n",
    "                    text_content.append(f\"Page {page_num + 1}: [Error extracting content]\")\n",
    "        \n",
    "        return \"\\n\\n\".join(text_content)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to extract text from {pdf_path}: {str(e)}\")\n",
    "        return f\"Error processing file {os.path.basename(pdf_path)}: {str(e)}\"\n",
    "\n",
    "# New function to extract text from plain text files\n",
    "def extract_text_from_txt(txt_path):\n",
    "    \"\"\"Extract text from a plain text file with error handling.\"\"\"\n",
    "    try:\n",
    "        with open(txt_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "            \n",
    "        # Optional: Add line numbering or structure to the content\n",
    "        lines = content.split('\\n')\n",
    "        text_content = []\n",
    "        \n",
    "        for i, line in enumerate(lines):\n",
    "            if line.strip():  # Skip empty lines\n",
    "                text_content.append(f\"Line {i+1}: {line}\")\n",
    "        \n",
    "        return \"\\n\".join(text_content)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to extract text from {txt_path}: {str(e)}\")\n",
    "        return f\"Error processing file {os.path.basename(txt_path)}: {str(e)}\"\n",
    "\n",
    "# Enhanced function to load document files (PPTX, PDF, TXT)\n",
    "def load_document_files(directory_path):\n",
    "    \"\"\"Load all document files (PPTX, PDF, TXT) from a directory and extract their text.\"\"\"\n",
    "    try:\n",
    "        # Check if directory exists\n",
    "        if not os.path.exists(directory_path):\n",
    "            logger.error(f\"Directory not found: {directory_path}\")\n",
    "            raise FileNotFoundError(f\"Directory not found: {directory_path}\")\n",
    "        \n",
    "        # Find all files of the specified types\n",
    "        pptx_files = glob.glob(os.path.join(directory_path, \"*.pptx\"))\n",
    "        pdf_files = glob.glob(os.path.join(directory_path, \"*.pdf\"))\n",
    "        txt_files = glob.glob(os.path.join(directory_path, \"*.txt\"))\n",
    "        \n",
    "        all_files = pptx_files + pdf_files + txt_files\n",
    "        \n",
    "        if not all_files:\n",
    "            logger.warning(f\"No document files found in {directory_path}\")\n",
    "        \n",
    "        all_text = []\n",
    "        file_sources = []\n",
    "        processed_count = 0\n",
    "        \n",
    "        # Process PowerPoint files\n",
    "        for file_path in pptx_files:\n",
    "            try:\n",
    "                file_name = os.path.basename(file_path)\n",
    "                logger.info(f\"Processing PPTX: {file_name}\")\n",
    "                text = extract_text_from_pptx(file_path)\n",
    "                # Add file source for better context\n",
    "                text = f\"Source: {file_name}\\n\\n{text}\"\n",
    "                all_text.append(text)\n",
    "                file_sources.append(file_name)\n",
    "                processed_count += 1\n",
    "            except Exception as file_error:\n",
    "                logger.error(f\"Error processing file {file_path}: {str(file_error)}\")\n",
    "        \n",
    "        # Process PDF files\n",
    "        for file_path in pdf_files:\n",
    "            try:\n",
    "                file_name = os.path.basename(file_path)\n",
    "                logger.info(f\"Processing PDF: {file_name}\")\n",
    "                text = extract_text_from_pdf(file_path)\n",
    "                # Add file source for better context\n",
    "                text = f\"Source: {file_name}\\n\\n{text}\"\n",
    "                all_text.append(text)\n",
    "                file_sources.append(file_name)\n",
    "                processed_count += 1\n",
    "            except Exception as file_error:\n",
    "                logger.error(f\"Error processing file {file_path}: {str(file_error)}\")\n",
    "        \n",
    "        # Process TXT files\n",
    "        for file_path in txt_files:\n",
    "            try:\n",
    "                file_name = os.path.basename(file_path)\n",
    "                logger.info(f\"Processing TXT: {file_name}\")\n",
    "                text = extract_text_from_txt(file_path)\n",
    "                # Add file source for better context\n",
    "                text = f\"Source: {file_name}\\n\\n{text}\"\n",
    "                all_text.append(text)\n",
    "                file_sources.append(file_name)\n",
    "                processed_count += 1\n",
    "            except Exception as file_error:\n",
    "                logger.error(f\"Error processing file {file_path}: {str(file_error)}\")\n",
    "        \n",
    "        logger.info(f\"Processed {processed_count} out of {len(all_files)} document files\")\n",
    "        return all_text, file_sources\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load document files: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Process and split text\n",
    "def process_text(texts):\n",
    "    \"\"\"Split the text into smaller chunks for better retrieval with error handling.\"\"\"\n",
    "    try:\n",
    "        if not texts:\n",
    "            logger.warning(\"No texts provided for processing\")\n",
    "            return []\n",
    "            \n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=200,\n",
    "            length_function=len,\n",
    "        )\n",
    "        \n",
    "        all_splits = []\n",
    "        for i, text in enumerate(texts):\n",
    "            try:\n",
    "                splits = text_splitter.split_text(text)\n",
    "                all_splits.extend(splits)\n",
    "            except Exception as text_error:\n",
    "                logger.warning(f\"Error splitting text chunk {i}: {str(text_error)}\")\n",
    "        \n",
    "        logger.info(f\"Created {len(all_splits)} text chunks from {len(texts)} documents\")\n",
    "        return all_splits\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to process texts: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Create Vector Store\n",
    "def create_vector_store(text_chunks):\n",
    "    \"\"\"Create a vector store using FAISS for efficient similarity search with error handling.\"\"\"\n",
    "    try:\n",
    "        if not text_chunks:\n",
    "            logger.warning(\"No text chunks provided for vector store creation\")\n",
    "            raise ValueError(\"No text chunks provided for vector store creation\")\n",
    "            \n",
    "        # Create directory for vector store if it doesn't exist\n",
    "        os.makedirs(VECTOR_STORE_DIR, exist_ok=True)\n",
    "        \n",
    "        logger.info(f\"Initializing embeddings model: {EMBEDDING_MODEL}\")\n",
    "        embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=EMBEDDING_MODEL,\n",
    "            model_kwargs={'device': 'cpu'}\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Creating vector store with {len(text_chunks)} chunks\")\n",
    "        vector_store = FAISS.from_texts(text_chunks, embeddings)\n",
    "        \n",
    "        # Save the vector store for future use\n",
    "        vector_store_path = os.path.join(VECTOR_STORE_DIR, \"faiss_index\")\n",
    "        vector_store.save_local(vector_store_path)\n",
    "        logger.info(f\"Vector store saved to {vector_store_path}\")\n",
    "        \n",
    "        return vector_store\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to create vector store: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Try to load existing vector store or create a new one\n",
    "def get_vector_store(text_chunks=None):\n",
    "    \"\"\"Get vector store - either load existing one or create new one.\"\"\"\n",
    "    try:\n",
    "        vector_store_path = os.path.join(VECTOR_STORE_DIR, \"faiss_index\")\n",
    "        \n",
    "        if os.path.exists(vector_store_path) and not text_chunks:\n",
    "            try:\n",
    "                logger.info(\"Loading existing vector store\")\n",
    "                embeddings = HuggingFaceEmbeddings(\n",
    "                    model_name=EMBEDDING_MODEL,\n",
    "                    model_kwargs={'device': 'cpu'}\n",
    "                )\n",
    "                vector_store = FAISS.load_local(vector_store_path, embeddings)\n",
    "                logger.info(\"Vector store loaded successfully\")\n",
    "                return vector_store\n",
    "            except Exception as load_error:\n",
    "                logger.warning(f\"Failed to load vector store: {str(load_error)}. Will create new one.\")\n",
    "        \n",
    "        if text_chunks:\n",
    "            return create_vector_store(text_chunks)\n",
    "        else:\n",
    "            raise ValueError(\"No text chunks provided and no existing vector store found\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error with vector store: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Set Up Gemini LLM\n",
    "def setup_gemini_llm():\n",
    "    \"\"\"Set up the Gemini Pro LLM via API with error handling.\"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Setting up Gemini LLM with model: {MODEL_NAME}\")\n",
    "        llm = ChatGoogleGenerativeAI(\n",
    "            model=MODEL_NAME,\n",
    "            temperature=TEMPERATURE,\n",
    "            top_p=TOP_P,\n",
    "            max_output_tokens=MAX_TOKENS,\n",
    "            convert_system_message_to_human=True\n",
    "        )\n",
    "        \n",
    "        return llm\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to set up Gemini LLM: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Create Conversational Retrieval Chain\n",
    "def create_chatbot(vector_store, llm, memory):\n",
    "    \"\"\"Create the conversational retrieval chain with error handling.\"\"\"\n",
    "    try:        \n",
    "        qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "            llm=llm,\n",
    "            retriever=vector_store.as_retriever(search_kwargs={\"k\": 3}),\n",
    "            memory=memory,\n",
    "            return_source_documents=True\n",
    "        )\n",
    "        \n",
    "        logger.info(\"Conversational retrieval chain created successfully\")\n",
    "        return qa_chain\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to create conversational chain: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Update the initialize_chatbot function to use the new load_document_files function\n",
    "def initialize_chatbot(doc_directory=None):\n",
    "    \"\"\"Initialize the chatbot with proper error handling.\"\"\"\n",
    "    try:\n",
    "        # Use environment variable if no directory provided\n",
    "        if doc_directory is None:\n",
    "            doc_directory = LECTURES_DIR\n",
    "        \n",
    "        # Ensure the lectures directory exists\n",
    "        if not os.path.exists(doc_directory):\n",
    "            os.makedirs(doc_directory, exist_ok=True)\n",
    "            logger.warning(f\"Created lectures directory: {doc_directory}\")\n",
    "        \n",
    "        # Check if we have an existing vector store\n",
    "        vector_store_path = os.path.join(VECTOR_STORE_DIR, \"faiss_index\")\n",
    "        \n",
    "        if os.path.exists(vector_store_path):\n",
    "            try:\n",
    "                # Try to load existing vector store\n",
    "                vector_store = get_vector_store()\n",
    "                logger.info(\"Using existing vector store\")\n",
    "                \n",
    "                # Just get the list of files\n",
    "                _, file_sources = load_document_files(doc_directory)\n",
    "            except Exception as vs_error:\n",
    "                logger.warning(f\"Error loading vector store: {str(vs_error)}. Creating new one.\")\n",
    "                # Process files and create new vector store\n",
    "                all_documents, file_sources = load_document_files(doc_directory)\n",
    "                logger.info(f\"Loaded {len(all_documents)} document files\")\n",
    "                \n",
    "                # Process and split the text\n",
    "                text_chunks = process_text(all_documents)\n",
    "                logger.info(f\"Created {len(text_chunks)} text chunks\")\n",
    "                \n",
    "                # Create vector store\n",
    "                vector_store = create_vector_store(text_chunks)\n",
    "        else:\n",
    "            # No existing vector store, so create one\n",
    "            all_documents, file_sources = load_document_files(doc_directory)\n",
    "            logger.info(f\"Loaded {len(all_documents)} document files\")\n",
    "            \n",
    "            # Process and split the text\n",
    "            text_chunks = process_text(all_documents)\n",
    "            logger.info(f\"Created {len(text_chunks)} text chunks\")\n",
    "            \n",
    "            # Create vector store\n",
    "            vector_store = create_vector_store(text_chunks)\n",
    "        \n",
    "        # Setup memory to maintain conversation history\n",
    "        memory = ConversationBufferMemory(\n",
    "            memory_key=\"chat_history\",\n",
    "            output_key=\"answer\",\n",
    "            return_messages=True\n",
    "        )\n",
    "        \n",
    "        # Set up LLM and QA chain\n",
    "        llm = setup_gemini_llm()\n",
    "        qa_chain = create_chatbot(vector_store, llm, memory)\n",
    "        logger.info(\"Chatbot initialized successfully\")\n",
    "        \n",
    "        return qa_chain, file_sources\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to initialize chatbot: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# Main chatbot response function for TUPLE format (the original format)\n",
    "def get_response(message, history):\n",
    "    \"\"\"Get response from the chatbot with error handling.\"\"\"\n",
    "    if not message.strip():\n",
    "        history = history or []\n",
    "        history.append((message, \"Please enter a question.\"))\n",
    "        return history\n",
    "        \n",
    "    try:\n",
    "        # Convert message to the format expected by the chain\n",
    "        logger.info(f\"Processing question: {message}\")\n",
    "        result = qa_chain({\"question\": message})\n",
    "        answer = result[\"answer\"]\n",
    "        \n",
    "        # Format source information\n",
    "        sources = []\n",
    "        for doc in result[\"source_documents\"]:\n",
    "            try:\n",
    "                source = doc.metadata.get(\"source\", \"Unknown source\")\n",
    "                if source not in sources and \"Source:\" in doc.page_content:\n",
    "                    source_line = [line for line in doc.page_content.split('\\n') if 'Source:' in line]\n",
    "                    if source_line:\n",
    "                        source = source_line[0].replace('Source:', '').strip()\n",
    "                \n",
    "                if source not in sources:\n",
    "                    sources.append(source)\n",
    "            except Exception as doc_error:\n",
    "                logger.warning(f\"Error processing document source: {str(doc_error)}\")\n",
    "        \n",
    "        # Add sources to the response\n",
    "        if sources:\n",
    "            source_text = \"\\n\\n**Sources:**\\n\"\n",
    "            for src in sources:\n",
    "                if src and src != \"Unknown source\":\n",
    "                    source_text += f\"- {src}\\n\"\n",
    "            \n",
    "            if source_text != \"\\n\\n**Sources:**\\n\":\n",
    "                answer += source_text\n",
    "        \n",
    "        logger.info(\"Successfully generated response\")\n",
    "        \n",
    "        # Update history with the new message pair\n",
    "        history = history or []\n",
    "        history.append((message, answer))\n",
    "        return history\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error generating response: {str(e)}\")\n",
    "        error_message = f\"Error: {str(e)}\\nPlease try again with a different question.\"\n",
    "        history = history or []\n",
    "        history.append((message, error_message))\n",
    "        return history\n",
    "\n",
    "# Error response for the TUPLE format\n",
    "def error_response(message, history):\n",
    "    history = history or []\n",
    "    history.append((message, \"Chatbot initialization failed. Please check the configuration and restart the application.\"))\n",
    "    return history\n",
    "\n",
    "# Try to initialize the chatbot - wrapped in try/except for robust error handling\n",
    "try:\n",
    "    # Initialize the chatbot\n",
    "    qa_chain, available_files = initialize_chatbot()\n",
    "    logger.info(\"Chatbot initialization complete\")\n",
    "except Exception as init_error:\n",
    "    logger.critical(f\"Critical error initializing chatbot: {str(init_error)}\")\n",
    "    # Set these to None so we can handle in the UI\n",
    "    qa_chain = None\n",
    "    available_files = []\n",
    "\n",
    "# Define Gradio interface\n",
    "with gr.Blocks(css=\"footer {visibility: hidden}\") as demo:\n",
    "    gr.Markdown(\n",
    "        \"\"\"\n",
    "        # CTSE Lecture Notes Chatbot\n",
    "        \n",
    "        Welcome to the **Current Trends in Software Engineering** Lecture Notes Chatbot! \n",
    "        Ask any questions about the lecture content, and I'll try to answer based on the available lecture notes.\n",
    "\n",
    "        Supported file formats: PowerPoint (PPTX), PDF, and Text (TXT) files.\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    # Show error message if initialization failed\n",
    "    if qa_chain is None:\n",
    "        gr.Markdown(\n",
    "            \"\"\"\n",
    "            ⚠️ **Error: Chatbot initialization failed.**\n",
    "            \n",
    "            Please check the log file for details and make sure:\n",
    "            1. Your .env file is properly configured\n",
    "            2. The Google API key is valid\n",
    "            3. Lecture files are accessible\n",
    "            \n",
    "            The chatbot interface is still available but may not function correctly.\n",
    "            \"\"\"\n",
    "        )\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=4):\n",
    "            # IMPORTANT: Using standard chatbot without type=\"messages\"\n",
    "            chatbot = gr.Chatbot(height=500)\n",
    "            with gr.Row():\n",
    "                msg = gr.Textbox(\n",
    "                    placeholder=\"Ask a question about CTSE lectures...\",\n",
    "                    show_label=False,\n",
    "                    scale=9\n",
    "                )\n",
    "                submit = gr.Button(\"Send\", scale=1)\n",
    "            clear = gr.Button(\"Clear Chat\")\n",
    "        \n",
    "        with gr.Column(scale=1):\n",
    "            gr.Markdown(\"### Available Lecture Notes\")\n",
    "            file_list = gr.Dataframe(\n",
    "                headers=[\"Lecture Files\"],\n",
    "                datatype=[\"str\"],\n",
    "                value=[[file] for file in available_files]\n",
    "            )\n",
    "    \n",
    "    # Set up event handlers - only if chatbot was initialized successfully\n",
    "    if qa_chain is not None:\n",
    "        msg.submit(\n",
    "            get_response,\n",
    "            [msg, chatbot],\n",
    "            [chatbot],\n",
    "            queue=False\n",
    "        ).then(\n",
    "            lambda: \"\",\n",
    "            None,\n",
    "            [msg],\n",
    "            queue=False\n",
    "        )\n",
    "        \n",
    "        submit.click(\n",
    "            get_response,\n",
    "            [msg, chatbot],\n",
    "            [chatbot],\n",
    "            queue=False\n",
    "        ).then(\n",
    "            lambda: \"\",\n",
    "            None,\n",
    "            [msg],\n",
    "            queue=False\n",
    "        )\n",
    "    else:\n",
    "        # Display an error message when attempted to use\n",
    "        msg.submit(\n",
    "            error_response,\n",
    "            [msg, chatbot],\n",
    "            [chatbot],\n",
    "            queue=False\n",
    "        ).then(\n",
    "            lambda: \"\",\n",
    "            None,\n",
    "            [msg],\n",
    "            queue=False\n",
    "        )\n",
    "        \n",
    "        submit.click(\n",
    "            error_response,\n",
    "            [msg, chatbot],\n",
    "            [chatbot],\n",
    "            queue=False\n",
    "        ).then(\n",
    "            lambda: \"\",\n",
    "            None,\n",
    "            [msg],\n",
    "            queue=False\n",
    "        )\n",
    "    \n",
    "    clear.click(lambda: [], None, chatbot, queue=False)\n",
    "\n",
    "    gr.Markdown(\n",
    "        \"\"\"\n",
    "        ### About This Chatbot\n",
    "        This chatbot is powered by Google's Gemini AI and uses vector embeddings to search through CTSE lecture slides.\n",
    "        It can help answer questions about cloud computing, microservices, DevOps, and other topics covered in the course.\n",
    "        \n",
    "        **Note**: The chatbot's knowledge is limited to the content of the lecture slides.\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "# Launch the Gradio interface\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        logger.info(\"Starting Gradio interface\")\n",
    "        demo.launch(share=True)\n",
    "    except Exception as launch_error:\n",
    "        logger.critical(f\"Failed to launch Gradio interface: {str(launch_error)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
