{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a630f56",
   "metadata": {},
   "source": [
    "# CTSE Lecture Notes Chatbot using Gemini API\n",
    "# SE4010 - Current Trends in Software Engineering Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106e82d5",
   "metadata": {},
   "source": [
    "## 1. Install Required Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635c4e52",
   "metadata": {},
   "source": [
    "# Run this cell to install all necessary packages - remove the comments\n",
    "# !pip install langchain langchain_community python-pptx sentence-transformers faiss-cpu\n",
    "# !pip install google-generativeai langchain-google-genai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cd0524",
   "metadata": {},
   "source": [
    "## 2. Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70033fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from pptx import Presentation\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214bb365",
   "metadata": {},
   "source": [
    "## 3. Configure Gemini API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f74cfaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your Gemini API key (you'll need to get this from Google AI Studio)\n",
    "GOOGLE_API_KEY = \"AIzaSyAKvcPDDIQUJW6qLZDoGWU4PJgJX76eOOM\"  # Replace with your actual API key\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316620ef",
   "metadata": {},
   "source": [
    "## 4./5.Function to Extract Text from PowerPoint Files & Load PowerPoint Files from a Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33e487b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: AWS User Groups Colombo - Introduction to AWS Cloud Platform.pptx\n",
      "Processing: CAP Theorem.pptx\n",
      "Processing: Cloud Computing 101.pptx\n",
      "Processing: Cloud Design Patterns - 1.pptx\n",
      "Processing: Cloud Design Patterns - 2.pptx\n",
      "Processing: Containers 101.pptx\n",
      "Processing: Intro to DevOps and Beyond.pptx\n",
      "Processing: Introduction to Microservices.pptx\n",
      "Processing: Key Essentials for Building Application in Cloud.pptx\n",
      "Processing: Lecture 2 - Part 1.pptx\n",
      "Processing: Lecture 2 - Part 2.pptx\n",
      "Processing: Microservice Design Patterns.pptx\n",
      "Loaded 12 PowerPoint files\n"
     ]
    }
   ],
   "source": [
    "## 4. Function to Extract Text from PowerPoint Files\n",
    "\n",
    "def extract_text_from_pptx(pptx_path):\n",
    "    \"\"\"Extract text from a PowerPoint file.\"\"\"\n",
    "    prs = Presentation(pptx_path)\n",
    "    text_content = []\n",
    "    \n",
    "    # Extract slide number for context\n",
    "    slide_number = 1\n",
    "    \n",
    "    for slide in prs.slides:\n",
    "        slide_text = f\"Slide {slide_number}: \"\n",
    "        for shape in slide.shapes:\n",
    "            if hasattr(shape, \"text\"):\n",
    "                slide_text += shape.text + \" \"\n",
    "        text_content.append(slide_text.strip())\n",
    "        slide_number += 1\n",
    "    \n",
    "    return \"\\n\\n\".join(text_content)\n",
    "\n",
    "## 5. Load PowerPoint Files from a Directory\n",
    "\n",
    "def load_pptx_files(directory_path):\n",
    "    \"\"\"Load all PowerPoint files from a directory and extract their text.\"\"\"\n",
    "    pptx_files = glob.glob(os.path.join(directory_path, \"*.pptx\"))\n",
    "    all_text = []\n",
    "    \n",
    "    for file_path in pptx_files:\n",
    "        file_name = os.path.basename(file_path)\n",
    "        print(f\"Processing: {file_name}\")\n",
    "        text = extract_text_from_pptx(file_path)\n",
    "        # Add file source for better context\n",
    "        text = f\"Source: {file_name}\\n\\n{text}\"\n",
    "        all_text.append(text)\n",
    "    \n",
    "    return all_text\n",
    "\n",
    "# Set the path to your folder containing PPTX lecture notes\n",
    "pptx_directory = \"./lectures\"  # Change this to your actual directory path\n",
    "\n",
    "# Uncomment the following line when ready to process your files\n",
    "all_lecture_notes = load_pptx_files(pptx_directory)\n",
    "print(f\"Loaded {len(all_lecture_notes)} PowerPoint files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f72919",
   "metadata": {},
   "source": [
    "## 6. Process and Split the Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "286e32c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 113 text chunks\n"
     ]
    }
   ],
   "source": [
    "def process_text(texts):\n",
    "    \"\"\"Split the text into smaller chunks for better retrieval.\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,  # Adjust based on your content\n",
    "        chunk_overlap=200,\n",
    "        length_function=len,\n",
    "    )\n",
    "    \n",
    "    all_splits = []\n",
    "    for text in texts:\n",
    "        splits = text_splitter.split_text(text)\n",
    "        all_splits.extend(splits)\n",
    "    \n",
    "    return all_splits\n",
    "\n",
    "# Uncomment when ready to process your files\n",
    "text_chunks = process_text(all_lecture_notes)\n",
    "print(f\"Created {len(text_chunks)} text chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5656fb4d",
   "metadata": {},
   "source": [
    "## 7. Create Vector Store for Similarity Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "933050ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store created successfully\n"
     ]
    }
   ],
   "source": [
    "def create_vector_store(text_chunks):\n",
    "    \"\"\"Create a vector store using FAISS for efficient similarity search.\"\"\"\n",
    "    # Using a free, locally-running embedding model\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"all-MiniLM-L6-v2\",  # A lightweight model that works well for semantic search\n",
    "        model_kwargs={'device': 'cpu'}\n",
    "    )\n",
    "    \n",
    "    vector_store = FAISS.from_texts(text_chunks, embeddings)\n",
    "    return vector_store\n",
    "\n",
    "# Uncomment when ready to create your vector store\n",
    "vector_store = create_vector_store(text_chunks)\n",
    "print(\"Vector store created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127e19eb",
   "metadata": {},
   "source": [
    "## 8. Set Up Gemini LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f978fb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_gemini_llm():\n",
    "    \"\"\"Set up the Gemini Pro LLM via API.\"\"\"\n",
    "    llm = ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-2.0-flash\",\n",
    "        temperature=0.7,\n",
    "        top_p=0.85,\n",
    "        max_output_tokens=1024,\n",
    "        convert_system_message_to_human=True\n",
    "    )\n",
    "    \n",
    "    return llm\n",
    "\n",
    "# Setup memory to maintain conversation history\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    output_key=\"answer\",  # Add this line to specify which output to store\n",
    "    return_messages=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd12b22",
   "metadata": {},
   "source": [
    "## 9. Create the Conversational Retrieval Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30af8b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot is ready!\n"
     ]
    }
   ],
   "source": [
    "def create_chatbot(vector_store, llm):\n",
    "    \"\"\"Create the conversational retrieval chain.\"\"\"\n",
    "    qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "        llm=llm,\n",
    "        retriever=vector_store.as_retriever(search_kwargs={\"k\": 3}),\n",
    "        memory=memory,\n",
    "        return_source_documents=True  # To show where the information comes from\n",
    "    )\n",
    "    \n",
    "    return qa_chain\n",
    "\n",
    "# Uncomment when ready to set up your chatbot\n",
    "llm = setup_gemini_llm()\n",
    "qa_chain = create_chatbot(vector_store, llm)\n",
    "print(\"Chatbot is ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74eff838",
   "metadata": {},
   "source": [
    "## 10. Run the Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ace6bc83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pasan\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_google_genai\\chat_models.py:357: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What are the main topics covered in CTSE?\n",
      "\n",
      "Answer: I don't see an answer to the question \"What are the main topics covered in CTSE?\" in the given context.\n",
      "\n",
      "Sources:\n",
      "- Unknown source\n",
      "- Unknown source\n",
      "- Unknown source\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'What are the main topics covered in CTSE?',\n",
       " 'chat_history': [HumanMessage(content='What are the main topics covered in CTSE?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='I don\\'t see an answer to the question \"What are the main topics covered in CTSE?\" in the given context.', additional_kwargs={}, response_metadata={})],\n",
       " 'answer': 'I don\\'t see an answer to the question \"What are the main topics covered in CTSE?\" in the given context.',\n",
       " 'source_documents': [Document(id='7948a28e-b058-4319-8b6c-ddff45a9853d', metadata={}, page_content='Slide 21: References  https://www.cloudflare.com/learning/cloud/what-is-the-cloud/ \\nhttps://www.redhat.com/en/topics/cloud-computing/what-is-iaas \\nhttps://www.redhat.com/en/topics/cloud-computing/what-is-paas \\nhttps://www.redhat.com/en/topics/cloud-computing/what-is-saas\\nCloud Computing: Concepts, Technology & Architecture, Thomas Erl, et al., Prentice‐Hall, 2013,\\nThe Datacenter as a Computer – Designing Warehouse‐Scale Machines, 3rd Edition, Morgan & Claypool Publishers, 2019\\nCloud design patterns: Prescriptive architecture guidance for cloud applications, Homer, Alex, et al. , 2014.'),\n",
       "  Document(id='0b6288c8-d3ef-47c3-98be-6c1589691e1f', metadata={}, page_content='Slide 16:  Event Sourcing Pattern\\n\\nSlide 17: Event Sourcing Pattern (Cont.)  \\uf070 \\uf070 \\uf070 Cons  \\uf06e Record full series of events than current state Pros  \\uf06e \\uf06e \\uf06e \\uf06e \\uf06e Consistency relaxed  Avoid requirement to synchronize data  \\uf070 \\uf070 Scalability \\nResponsiveness \\nProvide consistency for transactional data Full audit trails  Traditional Create, Read, Update, & Delete (CRUD) model too slow \\nImprove performance with eventual consistency\\n\\nSlide 18:  External Configuration Store Pattern  \\uf070 Move configuration information out of application deployment package to a central location\\n\\nSlide 19:  Federated Identity Pattern  \\uf070 Delegate authentication to an external identity provider\\n\\nSlide 20:  Health Endpoint Monitoring Pattern  \\uf070 Functional checks within an application that external tools can access through exposed endpoints at regular intervals'),\n",
       "  Document(id='a8e367d7-7b9f-4b38-8b88-6f455e9d9af4', metadata={}, page_content='Slide 4: Control types in Shared Responsibility Model Inherited Controls: Controls fully managed by CSP (e.g., physical and environmental controls).\\nShared Controls: Controls that apply to both CSP and customers but in different contexts (e.g., patch management, configuration management).\\nPatch Management – CSP is responsible for patching and fixing flaws within the infrastructure, but customers are responsible for patching their guest OS and applications.\\nConfiguration Management – CSP maintains the configuration of its infrastructure devices, but a customer is responsible for configuring their own guest operating systems, databases, and applications.\\nAwareness & Training - CSP trains CSP’s employees, but a customer must train their own employees.\\nCustomer Specific Controls: Controls solely managed by the customer, depending on their applications and use of CSP services.\\n\\nSlide 5:')]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ask_question(qa_chain, question):\n",
    "    \"\"\"Ask a question to the chatbot.\"\"\"\n",
    "    try:\n",
    "        result = qa_chain({\"question\": question})\n",
    "        \n",
    "        print(\"Question:\", question)\n",
    "        print(\"\\nAnswer:\", result[\"answer\"])\n",
    "        print(\"\\nSources:\")\n",
    "        for doc in result[\"source_documents\"]:\n",
    "            print(\"-\", doc.metadata.get(\"source\", \"Unknown source\"))\n",
    "        \n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Example usage:\n",
    "ask_question(qa_chain, \"What are the main topics covered in CTSE?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835a60b4",
   "metadata": {},
   "source": [
    "## 11. Interactive Chat Interface\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535bd9bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the CTSE Lecture Notes Chatbot!\n",
      "Type 'exit' to end the conversation.\n"
     ]
    }
   ],
   "source": [
    "def interactive_chat():\n",
    "    \"\"\"Run an interactive chat session.\"\"\"\n",
    "    print(\"Welcome to the CTSE Lecture Notes Chatbot!\")\n",
    "    print(\"Type 'exit' to end the conversation.\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"\\nAsk a question: \")\n",
    "        if user_input.lower() == 'exit':\n",
    "            break\n",
    "        \n",
    "        try:\n",
    "            ask_question(qa_chain, user_input)\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            print(\"Please try another question.\")\n",
    "\n",
    "# Start the interactive chat when ready\n",
    "interactive_chat()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
