{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a630f56",
   "metadata": {},
   "source": [
    "# CTSE Lecture Notes Chatbot using Gemini API\n",
    "# SE4010 - Current Trends in Software Engineering Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "651aa459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: AWS User Groups Colombo - Introduction to AWS Cloud Platform.pptx\n",
      "Processing: CAP Theorem.pptx\n",
      "Processing: Cloud Computing 101.pptx\n",
      "Processing: Cloud Design Patterns - 1.pptx\n",
      "Processing: Cloud Design Patterns - 2.pptx\n",
      "Processing: Containers 101.pptx\n",
      "Processing: Intro to DevOps and Beyond.pptx\n",
      "Processing: Introduction to Microservices.pptx\n",
      "Processing: Key Essentials for Building Application in Cloud.pptx\n",
      "Processing: Lecture 2 - Part 1.pptx\n",
      "Processing: Lecture 2 - Part 2.pptx\n",
      "Processing: Microservice Design Patterns.pptx\n",
      "Loaded 12 PowerPoint files\n",
      "Created 113 text chunks\n",
      "Vector store created successfully\n",
      "Chatbot is ready!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pasan\\AppData\\Local\\Temp\\ipykernel_26456\\402648358.py:191: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(height=500)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "Could not create share link. Please check your internet connection or our status page: https://status.gradio.app.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pasan\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_google_genai\\chat_models.py:357: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n",
      "C:\\Users\\Pasan\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_google_genai\\chat_models.py:357: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n",
      "C:\\Users\\Pasan\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_google_genai\\chat_models.py:357: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n",
      "C:\\Users\\Pasan\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_google_genai\\chat_models.py:357: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n",
      "C:\\Users\\Pasan\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_google_genai\\chat_models.py:357: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n",
      "C:\\Users\\Pasan\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_google_genai\\chat_models.py:357: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n",
      "C:\\Users\\Pasan\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_google_genai\\chat_models.py:357: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n",
      "C:\\Users\\Pasan\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_google_genai\\chat_models.py:357: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n",
      "C:\\Users\\Pasan\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_google_genai\\chat_models.py:357: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n",
      "C:\\Users\\Pasan\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_google_genai\\chat_models.py:357: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n",
      "C:\\Users\\Pasan\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_google_genai\\chat_models.py:357: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n",
      "C:\\Users\\Pasan\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_google_genai\\chat_models.py:357: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n",
      "C:\\Users\\Pasan\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_google_genai\\chat_models.py:357: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n",
      "C:\\Users\\Pasan\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_google_genai\\chat_models.py:357: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n",
      "C:\\Users\\Pasan\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_google_genai\\chat_models.py:357: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n",
      "C:\\Users\\Pasan\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_google_genai\\chat_models.py:357: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n",
      "C:\\Users\\Pasan\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_google_genai\\chat_models.py:357: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n",
      "C:\\Users\\Pasan\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_google_genai\\chat_models.py:357: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n",
      "C:\\Users\\Pasan\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_google_genai\\chat_models.py:357: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n",
      "C:\\Users\\Pasan\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_google_genai\\chat_models.py:357: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n",
      "C:\\Users\\Pasan\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_google_genai\\chat_models.py:357: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n",
      "C:\\Users\\Pasan\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_google_genai\\chat_models.py:357: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n",
      "C:\\Users\\Pasan\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_google_genai\\chat_models.py:357: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n",
      "C:\\Users\\Pasan\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_google_genai\\chat_models.py:357: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n",
      "C:\\Users\\Pasan\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_google_genai\\chat_models.py:357: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n",
      "C:\\Users\\Pasan\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_google_genai\\chat_models.py:357: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n",
      "C:\\Users\\Pasan\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_google_genai\\chat_models.py:357: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n",
      "C:\\Users\\Pasan\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_google_genai\\chat_models.py:357: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n",
      "C:\\Users\\Pasan\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_google_genai\\chat_models.py:357: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n",
      "C:\\Users\\Pasan\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_google_genai\\chat_models.py:357: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n",
      "C:\\Users\\Pasan\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_google_genai\\chat_models.py:357: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import os\n",
    "import glob\n",
    "from pptx import Presentation\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Set your Gemini API key\n",
    "GOOGLE_API_KEY = \"AIzaSyAKvcPDDIQUJW6qLZDoGWU4PJgJX76eOOM\"  # Replace with your actual API key\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
    "\n",
    "# Function to extract text from PowerPoint files\n",
    "def extract_text_from_pptx(pptx_path):\n",
    "    \"\"\"Extract text from a PowerPoint file.\"\"\"\n",
    "    prs = Presentation(pptx_path)\n",
    "    text_content = []\n",
    "    \n",
    "    # Extract slide number for context\n",
    "    slide_number = 1\n",
    "    \n",
    "    for slide in prs.slides:\n",
    "        slide_text = f\"Slide {slide_number}: \"\n",
    "        for shape in slide.shapes:\n",
    "            if hasattr(shape, \"text\"):\n",
    "                slide_text += shape.text + \" \"\n",
    "        text_content.append(slide_text.strip())\n",
    "        slide_number += 1\n",
    "    \n",
    "    return \"\\n\\n\".join(text_content)\n",
    "\n",
    "# Load PowerPoint Files\n",
    "def load_pptx_files(directory_path):\n",
    "    \"\"\"Load all PowerPoint files from a directory and extract their text.\"\"\"\n",
    "    pptx_files = glob.glob(os.path.join(directory_path, \"*.pptx\"))\n",
    "    all_text = []\n",
    "    file_sources = []\n",
    "    \n",
    "    for file_path in pptx_files:\n",
    "        file_name = os.path.basename(file_path)\n",
    "        print(f\"Processing: {file_name}\")\n",
    "        text = extract_text_from_pptx(file_path)\n",
    "        # Add file source for better context\n",
    "        text = f\"Source: {file_name}\\n\\n{text}\"\n",
    "        all_text.append(text)\n",
    "        file_sources.append(file_name)\n",
    "    \n",
    "    return all_text, file_sources\n",
    "\n",
    "# Process and split text\n",
    "def process_text(texts):\n",
    "    \"\"\"Split the text into smaller chunks for better retrieval.\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        length_function=len,\n",
    "    )\n",
    "    \n",
    "    all_splits = []\n",
    "    for text in texts:\n",
    "        splits = text_splitter.split_text(text)\n",
    "        all_splits.extend(splits)\n",
    "    \n",
    "    return all_splits\n",
    "\n",
    "# Create Vector Store\n",
    "def create_vector_store(text_chunks):\n",
    "    \"\"\"Create a vector store using FAISS for efficient similarity search.\"\"\"\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"all-MiniLM-L6-v2\",\n",
    "        model_kwargs={'device': 'cpu'}\n",
    "    )\n",
    "    \n",
    "    vector_store = FAISS.from_texts(text_chunks, embeddings)\n",
    "    return vector_store\n",
    "\n",
    "# Set Up Gemini LLM\n",
    "def setup_gemini_llm():\n",
    "    \"\"\"Set up the Gemini Pro LLM via API.\"\"\"\n",
    "    llm = ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-2.0-flash\",\n",
    "        temperature=0.7,\n",
    "        top_p=0.85,\n",
    "        max_output_tokens=1024,\n",
    "        convert_system_message_to_human=True\n",
    "    )\n",
    "    \n",
    "    return llm\n",
    "\n",
    "# Create Conversational Retrieval Chain\n",
    "def create_chatbot(vector_store, llm, memory):\n",
    "    \"\"\"Create the conversational retrieval chain.\"\"\"\n",
    "    qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "        llm=llm,\n",
    "        retriever=vector_store.as_retriever(search_kwargs={\"k\": 3}),\n",
    "        memory=memory,\n",
    "        return_source_documents=True\n",
    "    )\n",
    "    \n",
    "    return qa_chain\n",
    "\n",
    "# Initialize chatbot\n",
    "def initialize_chatbot(pptx_directory=\"./lectures\"):\n",
    "    # Load the PowerPoint files\n",
    "    all_lecture_notes, file_sources = load_pptx_files(pptx_directory)\n",
    "    print(f\"Loaded {len(all_lecture_notes)} PowerPoint files\")\n",
    "    \n",
    "    # Process and split the text\n",
    "    text_chunks = process_text(all_lecture_notes)\n",
    "    print(f\"Created {len(text_chunks)} text chunks\")\n",
    "    \n",
    "    # Create vector store\n",
    "    vector_store = create_vector_store(text_chunks)\n",
    "    print(\"Vector store created successfully\")\n",
    "    \n",
    "    # Setup memory to maintain conversation history\n",
    "    memory = ConversationBufferMemory(\n",
    "        memory_key=\"chat_history\",\n",
    "        output_key=\"answer\",\n",
    "        return_messages=True\n",
    "    )\n",
    "    \n",
    "    # Set up LLM and QA chain\n",
    "    llm = setup_gemini_llm()\n",
    "    qa_chain = create_chatbot(vector_store, llm, memory)\n",
    "    print(\"Chatbot is ready!\")\n",
    "    \n",
    "    return qa_chain, file_sources\n",
    "\n",
    "# Main chatbot response function\n",
    "def get_response(message, history):\n",
    "    try:\n",
    "        # Convert message to the format expected by the chain\n",
    "        result = qa_chain({\"question\": message})\n",
    "        answer = result[\"answer\"]\n",
    "        \n",
    "        # Format source information\n",
    "        sources = []\n",
    "        for doc in result[\"source_documents\"]:\n",
    "            source = doc.metadata.get(\"source\", \"Unknown source\")\n",
    "            if source not in sources and \"Source:\" in doc.page_content:\n",
    "                source_line = [line for line in doc.page_content.split('\\n') if 'Source:' in line]\n",
    "                if source_line:\n",
    "                    source = source_line[0].replace('Source:', '').strip()\n",
    "            \n",
    "            if source not in sources:\n",
    "                sources.append(source)\n",
    "        \n",
    "        # Add sources to the response\n",
    "        if sources:\n",
    "            source_text = \"\\n\\n**Sources:**\\n\"\n",
    "            for src in sources:\n",
    "                if src and src != \"Unknown source\":\n",
    "                    source_text += f\"- {src}\\n\"\n",
    "            \n",
    "            if source_text != \"\\n\\n**Sources:**\\n\":\n",
    "                answer += source_text\n",
    "        \n",
    "        # Update history with the new message pair\n",
    "        history = history or []\n",
    "        history.append((message, answer))\n",
    "        return history\n",
    "    \n",
    "    except Exception as e:\n",
    "        error_message = f\"Error: {str(e)}\\nPlease try again with a different question.\"\n",
    "        history = history or []\n",
    "        history.append((message, error_message))\n",
    "        return history\n",
    "\n",
    "# Initialize the chatbot\n",
    "qa_chain, available_files = initialize_chatbot()\n",
    "\n",
    "# Define Gradio interface\n",
    "with gr.Blocks(css=\"footer {visibility: hidden}\") as demo:\n",
    "    gr.Markdown(\n",
    "        \"\"\"\n",
    "        # CTSE Lecture Notes Chatbot\n",
    "        \n",
    "        Welcome to the **Current Trends in Software Engineering** Lecture Notes Chatbot! \n",
    "        Ask any questions about the lecture content, and I'll try to answer based on the available lecture notes.\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=4):\n",
    "            chatbot = gr.Chatbot(height=500)\n",
    "            with gr.Row():\n",
    "                msg = gr.Textbox(\n",
    "                    placeholder=\"Ask a question about CTSE lectures...\",\n",
    "                    show_label=False,\n",
    "                    scale=9\n",
    "                )\n",
    "                submit = gr.Button(\"Send\", scale=1)\n",
    "            clear = gr.Button(\"Clear Chat\")\n",
    "        \n",
    "        with gr.Column(scale=1):\n",
    "            gr.Markdown(\"### Available Lecture Notes\")\n",
    "            file_list = gr.Dataframe(\n",
    "                headers=[\"Lecture Files\"],\n",
    "                datatype=[\"str\"],\n",
    "                value=[[file] for file in available_files]\n",
    "            )\n",
    "    \n",
    "    # Set up event handlers\n",
    "    msg.submit(\n",
    "        get_response,\n",
    "        [msg, chatbot],\n",
    "        [chatbot],\n",
    "        queue=False\n",
    "    ).then(\n",
    "        lambda: \"\",\n",
    "        None,\n",
    "        [msg],\n",
    "        queue=False\n",
    "    )\n",
    "    \n",
    "    submit.click(\n",
    "        get_response,\n",
    "        [msg, chatbot],\n",
    "        [chatbot],\n",
    "        queue=False\n",
    "    ).then(\n",
    "        lambda: \"\",\n",
    "        None,\n",
    "        [msg],\n",
    "        queue=False\n",
    "    )\n",
    "    \n",
    "    clear.click(lambda: [], None, chatbot, queue=False)\n",
    "\n",
    "    \n",
    "    gr.Markdown(\n",
    "        \"\"\"\n",
    "        ### About This Chatbot\n",
    "        This chatbot is powered by Google's Gemini AI and uses vector embeddings to search through CTSE lecture slides.\n",
    "        It can help answer questions about cloud computing, microservices, DevOps, and other topics covered in the course.\n",
    "        \n",
    "        **Note**: The chatbot's knowledge is limited to the content of the lecture slides.\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "# Launch the Gradio interface\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
